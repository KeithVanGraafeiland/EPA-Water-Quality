{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PROCESS LOGIC\n",
    "## First download the station data for the Gulf States (TX, LA, AL, MS, FL)\n",
    "## Then clip the stations to the watershed counties \n",
    "## Use the station names to build a URL and access the narrow results from the EPA Water Quality API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## URL Examples\n",
    "## EPA Water Quality Site:\n",
    "## https://www.waterqualitydata.us/\n",
    "\n",
    "## Download all the stations for USA states FL, AL, TX, MS, LA as a GEOJSON:\n",
    "## https://www.waterqualitydata.us/data/Station/search?countrycode=US&statecode=US%3A01&statecode=US%3A12&statecode=US%3A48&statecode=US%3A28&statecode=US%3A22&mimeType=geojsonv&zip=yes&providers=NWIS&providers=STEWARDS&providers=STORET\n",
    "\n",
    "## Download all the sample data for a given station (0800257-CC-05) as a CSV:\n",
    "## https://www.waterqualitydata.us/data/Result/search?siteid=0800257-CC-05&mimeType=csv&zip=yes&dataProfile=narrowResult&providers=NWIS&providers=STEWARDS&providers=STORET\n",
    "\n",
    "## CODE REFERENCE\n",
    "## Arcpy Reference: https://developers.arcgis.com/python/api-reference/\n",
    "## Time Sleep refernce: https://stackoverflow.com/questions/72590954/how-to-add-the-time-sleep-after-every-100-iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import arcgis\n",
    "from arcgis.gis import GIS\n",
    "#from arcgis.mapping import WebMap\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas\n",
    "import glob\n",
    "gis = GIS(\"home\")\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcgis.env.verbose = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = r'C:\\Temp\\EPA-Water-Quality' #TODO Change to your preferred directory\n",
    "TODAY = (datetime.today())\n",
    "CURRENT_DATE = (TODAY.strftime('%Y%m%d'))\n",
    "#WATER_QUALITY_STATIONS_URL = 'https://www.waterqualitydata.us/data/Station/search?countrycode=US&statecode=US%3A01&countycode=US%3A01%3A003&mimeType=geojson&zip=yes&providers=NWIS&providers=STEWARDS&providers=STORET'\n",
    "WATER_QUALITY_STATIONS_URL = 'https://www.waterqualitydata.us/data/Station/search?countrycode=US&statecode=US%3A01&statecode=US%3A12&statecode=US%3A22&statecode=US%3A28&statecode=US%3A48&mimeType=geojson&zip=yes&providers=NWIS&providers=STEWARDS&providers=STORET'\n",
    "WORKING_DIRECTORY = ROOT + '\\\\working_' + str(CURRENT_DATE) + '\\\\'\n",
    "WATER_QUALITY_STATIONS_DIRECTORY = WORKING_DIRECTORY + '\\\\stations\\\\'\n",
    "WATER_QUALITY_STATIONS_ZIP = WATER_QUALITY_STATIONS_DIRECTORY + 'water_quality_stations.zip'\n",
    "WORKING_STATIONS_JSON = WORKING_DIRECTORY + 'station.geojson'\n",
    "WORKING_STATIONS_SHP  = WORKING_DIRECTORY + 'station.shp'\n",
    "CLIPPED_STATIONS = WORKING_DIRECTORY + 'goma_stations.shp'\n",
    "COASTAL_COUNTIES = WORKING_DIRECTORY + 'coastal_watershed_counties.shp'\n",
    "CSV_WORKING_DIRECTORY = WORKING_DIRECTORY + 'csv\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(ROOT):\n",
    "    os.makedirs(ROOT)\n",
    "if not os.path.exists(WORKING_DIRECTORY):\n",
    "    os.makedirs(WORKING_DIRECTORY)\n",
    "if not os.path.exists(CSV_WORKING_DIRECTORY):\n",
    "    os.makedirs(CSV_WORKING_DIRECTORY)\n",
    "if not os.path.exists(WATER_QUALITY_STATIONS_DIRECTORY):\n",
    "    os.makedirs(WATER_QUALITY_STATIONS_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clip the stations to the specified polygon boundary \n",
    "\n",
    "## coastal_watershed_counties item id = ff61c0c5154d4791ae51f00c4e4a3a98\n",
    "coastal_watershed_counties_item = gis.content.get('ff61c0c5154d4791ae51f00c4e4a3a98')\n",
    "coastal_watershed_counties_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_watershed_counties_layer = coastal_watershed_counties_item.layers[0]\n",
    "coastal_watershed_counties_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(WATER_QUALITY_STATIONS_URL, WATER_QUALITY_STATIONS_ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(WATER_QUALITY_STATIONS_ZIP, 'r') as zip_ref:\n",
    "    zip_ref.extractall(WORKING_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.conversion.JSONToFeatures(WORKING_STATIONS_JSON, WORKING_STATIONS_SHP, geometry_type=\"POINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.analysis.PairwiseDissolve(\n",
    "    in_features=\"https://services.arcgis.com/bDAhvQYMG4WL8O5o/arcgis/rest/services/coastal_watershed_counties_2010/FeatureServer/0\",\n",
    "    out_feature_class=COASTAL_COUNTIES,\n",
    "    dissolve_field=None,\n",
    "    statistics_fields=None,\n",
    "    multi_part=\"MULTI_PART\",\n",
    "    concatenation_separator=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.analysis.Clip(WORKING_STATIONS_SHP, COASTAL_COUNTIES, CLIPPED_STATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.management.CalculateGeometryAttributes(CLIPPED_STATIONS, [[\"X\", \"POINT_X\"], [\"Y\", \"POINT_Y\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_df = pandas.DataFrame.spatial.from_featureclass(CLIPPED_STATIONS)\n",
    "stations_df.set_index('Monitoring')\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in stations_df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the station list to get the sample data for each station\n",
    "\n",
    "valueList = []  # array to hold list of values collected\n",
    "valueSet = set()  # set to hold values to test against to get list\n",
    "rows = arcpy.SearchCursor(WORKING_STATIONS_SHP)\n",
    "field = \"Monitoring\"\n",
    "for row in rows:\n",
    "    value = row.getValue(field)\n",
    "    print('Processing Station ' + value)\n",
    "    old_file = os.path.join(CSV_WORKING_DIRECTORY, 'narrowresult.csv')\n",
    "    new_file = os.path.join(CSV_WORKING_DIRECTORY, value +'.csv')\n",
    "    sample_data_zip = CSV_WORKING_DIRECTORY + str(value) + '_' + CURRENT_DATE + '_sample_narrow.zip'\n",
    "    station_narrow_url = 'https://www.waterqualitydata.us/data/Result/search?siteid=' + value + '&mimeType=csv&zip=yes&dataProfile=narrowResult&providers=NWIS&providers=STEWARDS&providers=STORET'\n",
    "    urllib.request.urlretrieve(station_narrow_url, sample_data_zip)\n",
    "    with zipfile.ZipFile(sample_data_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(CSV_WORKING_DIRECTORY)\n",
    "    os.rename(old_file, new_file)\n",
    "    os.remove(sample_data_zip)\n",
    "    sample_df = pandas.read_csv(new_file)\n",
    "    sample_df.rename(columns={'MonitoringLocationIdentifier':'Monitoring'}, inplace=True)\n",
    "    sample_df.set_index('Monitoring')\n",
    "    result = pandas.merge(sample_df, stations_df, how='left')\n",
    "    result.drop(columns=['FID','SHAPE'], inplace=True)\n",
    "    result.to_csv(new_file)\n",
    "    # for row, count in enumerate(rows):\n",
    "    #     print(rows)\n",
    "    #     if row == 250:\n",
    "    #         time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's merge all the CSV files into one\n",
    "\n",
    "SAMPLE_NARROW_MERGED_CSV = CSV_WORKING_DIRECTORY + \"All_GOMA_Sample_Narrow_Results.csv\"\n",
    "joined_files = os.path.join(CSV_WORKING_DIRECTORY, \"*.csv\")\n",
    "# A list of all joined files is returned\n",
    "joined_list = glob.glob(joined_files)\n",
    "# Finally, the files are joined\n",
    "GOMA_NAROW_STATIONS_MERGED = pandas.concat(map(pandas.read_csv, joined_list), ignore_index=True)\n",
    "GOMA_NAROW_STATIONS_MERGED.to_csv(SAMPLE_NARROW_MERGED_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's create a shpaefile from the merged CSV file\n",
    "\n",
    "SHAPEFILE_NAME = 'sample_stations'\n",
    "SAMPLE_STATIONS_SHP = CSV_WORKING_DIRECTORY + SHAPEFILE_NAME + '.shp'\n",
    "SAMPLE_STATIONS = CSV_WORKING_DIRECTORY + SHAPEFILE_NAME\n",
    "\n",
    "arcpy.management.XYTableToPoint(\n",
    "    in_table=SAMPLE_NARROW_MERGED_CSV,\n",
    "    out_feature_class=SAMPLE_STATIONS_SHP,\n",
    "    x_field=\"X\",\n",
    "    y_field=\"Y\",\n",
    "    z_field=None,\n",
    "    coordinate_system='GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_WGS_1984\",SPHEROID[\"WGS_1984\",6378137.0,298.257223563]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]];-400 -400 1000000000;-100000 10000;-100000 10000;8.98315284119521E-09;0.001;0.001;IsHighPrecision'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's zip the shapefile so we can publish to arcgis online\n",
    "\n",
    "ZIPFILE_LOCATION = CSV_WORKING_DIRECTORY + SHAPEFILE_NAME + '.zip'\n",
    "\n",
    "shapefiles_list = [SAMPLE_STATIONS + \".cpg\",\n",
    "                   SAMPLE_STATIONS + \".dbf\",\n",
    "                   SAMPLE_STATIONS + \".prj\",\n",
    "                   SAMPLE_STATIONS + \".sbn\",\n",
    "                   SAMPLE_STATIONS + \".sbx\",\n",
    "                   SAMPLE_STATIONS + \".shp\",\n",
    "                   SAMPLE_STATIONS + \".shp.xml\",\n",
    "                   SAMPLE_STATIONS + \".shx\"]\n",
    "with zipfile.ZipFile(ZIPFILE_LOCATION, 'w') as zipMe:        \n",
    "    for file in shapefiles_list:\n",
    "        zipMe.write(file, compress_type=zipfile.ZIP_DEFLATED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
